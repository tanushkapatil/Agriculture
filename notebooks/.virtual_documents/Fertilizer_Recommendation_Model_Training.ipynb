import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, classification_report, 
                            confusion_matrix, ConfusionMatrixDisplay)
from sklearn.model_selection import GridSearchCV
import joblib
import plotly.express as px


# Set style for plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)


# Load Dataset
df = pd.read_csv('data/fertilizer_recommendation.csv')


# Display basic info
print("Dataset shape:", df.shape)
print("\nFirst 5 rows:")
display(df.head())


print("\nData types and missing values:")
print(df.info())


print("\nDescriptive statistics:")
display(df.describe())


#Check Class Distribution
print("\nFertilizer types distribution:")
print(df['Fertilizer Name'].value_counts())

plt.figure(figsize=(12, 6))
sns.countplot(data=df, y='Fertilizer Name', order=df['Fertilizer Name'].value_counts().index)
plt.title('Fertilizer Type Distribution')
plt.xlabel('Count')
plt.ylabel('Fertilizer Name')
plt.show()


print(df.columns.tolist())


# Check for similar column names (case-insensitive)
print([col for col in df.columns if 'temp' in col.lower()])
print([col for col in df.columns if 'humid' in col.lower()])
print([col for col in df.columns if col in ['N','P','K']])

# Then update your num_cols list accordingly


# Plot distributions of numerical features
# Use the exact column names from your DataFrame
num_cols = ['Temparature', 'Humidity ', 'Moisture', 'Nitrogen', 'Phosphorous', 'Potassium']

# Plot distributions
df[num_cols].hist(bins=20, figsize=(15, 10))
plt.tight_layout()
plt.show()


# Plot categorical features
cat_cols = ['Soil Type', 'Crop Type']

fig, axes = plt.subplots(1, 2, figsize=(15, 5))
for i, col in enumerate(cat_cols):
    sns.countplot(data=df, y=col, ax=axes[i], order=df[col].value_counts().index)
    axes[i].set_title(f'{col} Distribution')
plt.tight_layout()
plt.show()


# Pairplot for numerical features
sns.pairplot(df[num_cols + ['Fertilizer Name']], hue='Fertilizer Name', palette='viridis')
plt.suptitle('Pairplot of Numerical Features by Fertilizer Type', y=1.02)
plt.show()


#Data Preprocessing

# Encode categorical features
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
    print(f"{col} classes:", le.classes_)


# Encode target variable
le_fertilizer = LabelEncoder()
df['Fertilizer Name'] = le_fertilizer.fit_transform(df['Fertilizer Name'])
print("\nFertilizer classes:", le_fertilizer.classes_)


# Save encoders for later use
joblib.dump(label_encoders['Soil Type'], 'models/le_soil.pkl')
joblib.dump(label_encoders['Crop Type'], 'models/le_crop.pkl')
joblib.dump(le_fertilizer, 'models/le_fertilizer.pkl')


#Prepare Data for Modeling

# Split features and target
X = df.drop('Fertilizer Name', axis=1)
y = df['Fertilizer Name']


# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)


#Model Training

# Initialize Random Forest Classifier
rf = RandomForestClassifier(random_state=42)


# Define parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


# Perform GridSearchCV
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=1
)


print("Starting grid search...")
grid_search.fit(X_train, y_train)


# Get best parameters
best_params = grid_search.best_params_
print("\nBest parameters:", best_params)


# Train final model with best parameters
final_model = RandomForestClassifier(**best_params, random_state=42)
final_model.fit(X_train, y_train)


# Save the final model
joblib.dump(final_model, 'models/fertilizer_model.pkl')


#Model Evaluation

# Make predictions
y_pred = final_model.predict(X_test)


# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")


# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le_fertilizer.classes_))


# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=np.arange(len(le_fertilizer.classes_)))
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                             display_labels=le_fertilizer.classes_)
disp.plot(cmap='Blues', xticks_rotation='vertical')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()


# Get feature importances
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': final_model.feature_importances_
}).sort_values('Importance', ascending=False)


# Sort features by importance
feature_importances = feature_importances.sort_values('Importance', ascending=False)

# Then plot as above
plt.figure(figsize=(12, 8))
sns.barplot(
    data=feature_importances, 
    x='Importance', 
    y='Feature',
    hue='Feature',
    palette='viridis',
    legend=False
)
plt.title('Feature Importances (Sorted)')
plt.tight_layout()
plt.show()


# Interactive feature importance plot
fig = px.bar(feature_importances, 
             x='Importance', 
             y='Feature',
             orientation='h',
             title='Feature Importances',
             color='Importance',
             color_continuous_scale='viridis')
fig.show()





#Prepare sample data
sample_data = {
    'Temparature': 25.0,
    'Humidity ': 60.0,
    'Moisture': 50.0,
    'Soil Type': 'Loamy',  # Will be encoded
    'Crop Type': 'Wheat',  # Will be encoded
    'Nitrogen': 15.0,
    'Potassium': 12.0,
    'Phosphorous': 8.0
}


#Convert to DataFrame
sample_df = pd.DataFrame([sample_data])


#Encode categorical variables
sample_df['Soil Type'] = label_encoders['Soil Type'].transform(sample_df['Soil Type'])
sample_df['Crop Type'] = label_encoders['Crop Type'].transform(sample_df['Crop Type'])


#Ensure correct column order and numeric types
sample_features = sample_df[final_model.feature_names_in_].astype(float)


#Make prediction
try:
    prediction = final_model.predict(sample_features)
    pred_proba = final_model.predict_proba(sample_features)
    
    # Decode results
    fertilizer = le_fertilizer.inverse_transform(prediction)[0]
    print(f"Recommended Fertilizer: {fertilizer}")
    print("Confidence:", f"{pred_proba.max():.1%}")
    
except Exception as e:
    print("Prediction failed:", str(e))
    print("\nDebug Info:")
    print("Data types:", sample_features.dtypes)
    print("Sample data:\n", sample_features)


print("Features the model expects:", final_model.feature_names_in_)


print(f"\nSample Input: {sample_input}")
print(f"Recommended Fertilizer: {fertilizer}")
print(f"Confidence: {pred_proba.max():.1%}")


#Save Model Metadata

# Create metadata dictionary
metadata = {
    'model_type': 'RandomForestClassifier',
    'version': '1.0',
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d'),
    'input_features': list(X.columns),
    'target_classes': list(le_fertilizer.classes_),
    'performance': {
        'accuracy': accuracy,
        'best_params': best_params
    },
    'preprocessing': {
        'label_encoders': {
            'Soil Type': list(label_encoders['Soil Type'].classes_),
            'Crop Type': list(label_encoders['Crop Type'].classes_),
            'Fertilizer Name': list(le_fertilizer.classes_)
        }
    }
}


# Save metadata
joblib.dump(metadata, 'models/fertilizer_metadata.pkl')


#Conclusion

print("Model training completed successfully!")
print(f"Final model accuracy: {accuracy:.2%}")
print("Model and preprocessing artifacts saved to models/ directory")



